{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO:\n",
    "\n",
    "- Build pure ViT with new TP architecture\n",
    "    - Use lucidrains base, 2 layers\n",
    "    - Create my TP-Block\n",
    "- Extend to language\n",
    "- Explore use of T2T for vision pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# prepare image and text\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"hello world\"\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "\n",
    "inputs = processor(image, text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class RandomImageDataset(Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        assert n_samples % 2 == 0, \"n_samples must be an even number\"\n",
    "        self.n_samples = n_samples\n",
    "        self.labels = [0] * int(self.n_samples/2) + [1] * int(self.n_samples/2)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.n_samples/2:\n",
    "            x = torch.normal(0, 0.1, size=(3, 224, 224))\n",
    "        else:\n",
    "            x = torch.normal(1, 0.1, size=(3, 224, 224))\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "    \n",
    "train_ds = RandomImageDataset(100)\n",
    "train_datagen = DataLoader(train_ds, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tpr_block_vit import TP_ViT\n",
    "\n",
    "model = TP_ViT(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=2,\n",
    "    dim=768,\n",
    "    depth=2,\n",
    "    heads=8,\n",
    "    mlp_dim=3072\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE TO SELF:\n",
    "\n",
    "Query size should be size of dim head - aka not 768 but 768/n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 0.001\n",
      "[1,    10] loss: 0.000\n",
      "[1,    15] loss: 0.000\n",
      "[1,    20] loss: 0.000\n",
      "[1,    25] loss: 0.000\n",
      "[1,    30] loss: 0.000\n",
      "[1,    35] loss: 0.000\n",
      "[1,    40] loss: 0.000\n",
      "[1,    45] loss: 0.000\n",
      "[1,    50] loss: 0.000\n",
      "[2,     5] loss: 0.000\n",
      "[2,    10] loss: 0.000\n",
      "[2,    15] loss: 0.000\n",
      "[2,    20] loss: 0.000\n",
      "[2,    25] loss: 0.000\n",
      "[2,    30] loss: 0.000\n",
      "[2,    35] loss: 0.000\n",
      "[2,    40] loss: 0.000\n",
      "[2,    45] loss: 0.000\n",
      "[2,    50] loss: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_datagen, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 768, 197, 8])\n"
     ]
    }
   ],
   "source": [
    "'''Below is the correct matmul implementation\n",
    "for TPR with elementwise multiplication across the \n",
    "embedding dimension'''\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "f = torch.rand((7, 768, 197, 1)) # b, d, n, 1\n",
    "r = torch.rand((7, 768, 1, 8)) # b, d, 1, r\n",
    "\n",
    "out = torch.matmul(f, r)\n",
    "\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5913],\n",
      "          [0.5172],\n",
      "          [0.1101]],\n",
      "\n",
      "         [[0.9911],\n",
      "          [0.5339],\n",
      "          [0.3225]]]])\n",
      "tensor([[[[0.5453, 0.7955, 0.5617, 0.5944]],\n",
      "\n",
      "         [[0.9313, 0.0784, 0.9523, 0.9734]]]])\n",
      "tensor([[[[0.3224, 0.4704, 0.3321, 0.3515],\n",
      "          [0.2820, 0.4114, 0.2905, 0.3074],\n",
      "          [0.0600, 0.0876, 0.0618, 0.0654]],\n",
      "\n",
      "         [[0.9229, 0.0777, 0.9437, 0.9647],\n",
      "          [0.4972, 0.0419, 0.5084, 0.5197],\n",
      "          [0.3004, 0.0253, 0.3071, 0.3139]]]])\n"
     ]
    }
   ],
   "source": [
    "f = torch.rand((1, 2, 3, 1))\n",
    "r = torch.rand((1, 2, 1, 4))\n",
    "\n",
    "out = torch.matmul(f, r)\n",
    "\n",
    "print(f)\n",
    "print(r)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "role-1 token-1 = 0.3224, 0.9229\n",
    "\n",
    "Should be from 0.5453 * 0.5913, 0.9313 * 0.9911\n",
    "\n",
    "role-3 token-2 = 0.2905, 0.5084\n",
    "\n",
    "Should be from 0.5617 * 0.5172, 0.9523 * 0.5339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Parameter containing:\n",
      "tensor([[-0.2226, -0.0301, -0.2079,  0.2085, -0.0009, -0.0717,  0.1685, -0.2243,\n",
      "          0.0261, -0.1489, -0.1216, -0.0095],\n",
      "        [ 0.0681,  0.1024, -0.2068, -0.1476,  0.0485,  0.0148, -0.1064,  0.0873,\n",
      "          0.0888, -0.0157,  0.0807,  0.0339]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1804, 0.0633], requires_grad=True)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "net = TP_ViT(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=2,\n",
    "    dim=768,\n",
    "    depth=2,\n",
    "    heads=12,\n",
    "    mlp_dim=3072,\n",
    "    n_roles=12\n",
    ")\n",
    "x = torch.ones((1, 3, 224, 224))\n",
    "# compute the forward pass to create the computation graph\n",
    "y = net(x)\n",
    "\n",
    "# use computation graph to find all contributing tensors\n",
    "def get_contributing_params(y, top_level=True):\n",
    "    nf = y.grad_fn.next_functions if top_level else y.next_functions\n",
    "    for f, _ in nf:\n",
    "        try:\n",
    "            yield f.variable\n",
    "        except AttributeError:\n",
    "            pass  # node has no tensor\n",
    "        if f is not None:\n",
    "            yield from get_contributing_params(f, top_level=False)\n",
    "\n",
    "contributing_parameters = set(get_contributing_params(y))\n",
    "all_parameters = set(net.parameters())\n",
    "non_contributing = all_parameters - contributing_parameters\n",
    "print(non_contributing)  # returns the [999999.0] tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Stock ViT @ 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class PLModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.acc = torchmetrics.Accuracy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = model(x).logits\n",
    "        return logits\n",
    "        \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.loss(outputs, y)\n",
    "        acc = self.acc(self.softmax(outputs, dim=1), y)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "        \n",
    "        \n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "def transform(image):\n",
    "    inputs = feature_extractor(image, return_tensors=\"pt\")\n",
    "    return inputs.pixel_values.squeeze()\n",
    "\n",
    "valid_ds = torchvision.datasets.ImageFolder(\n",
    "    r'B:\\Datasets\\ImageNet2\\validation',\n",
    "    transform=transform\n",
    ")\n",
    "valid_datagen = DataLoader(valid_ds, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: B:\\PhD\\Projects\\tp-vilt\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86992b56dfa6468b83aa78b26a5c9200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator='gpu', devices=1)\n",
    "model = PLModel()\n",
    "model.eval;\n",
    "# trainer.test(model, valid_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(valid_ds[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87217d40363419aa48fc8cfd3aef9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8033, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = model.to('cuda')\n",
    "acc_fn = torchmetrics.Accuracy().to('cuda')\n",
    "\n",
    "accs = []\n",
    "for batch in tqdm(valid_datagen):\n",
    "    x, y = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(x.to('cuda')).logits\n",
    "    acc = acc_fn(torch.nn.functional.softmax(logits, dim=1), y.to('cuda'))\n",
    "    accs.append(acc)\n",
    "    \n",
    "print(sum(accs)/len(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d12415654094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_datagen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(valid_datagen[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
